---
title: "lab 6"
author: "Alicia Canales"
date: "2023-02-16"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE, warning =FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning =FALSE, message=FALSE)
```


```{r}
library(tidyverse)
library(janitor)
library(palmerpenguins)

### package for cluster analysis
library(NbClust)
library(factoextra)
library(dendextend)
library(ggdendro)

```

### Intro to cluster analysis - k-means, heirarchical

*Part1 K-mean clustering*
```{r}
ggplot(penguins) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm,
                 color = species,
                 shape = sex),
             size = 3,
             alpha = 0.7) + 
  scale_color_manual(values = c('orange', 'cyan4', 'darkmagenta')) # alpha is the transperancy of the dots

ggplot(penguins) + 
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g,
                 color = species,
                 shape = sex),
             size = 3,
             alpha = 0.7) +
  scale_color_manual(values = c('orange', 'cyan4', 'darkmagenta')) 

# We now have to pick clusters.. even though we know there are three penguins we should still pick them if we didnt 
```

### Create a complete, scaled version of the data
```{r}
# we are going to drop all rows w NAs
penguins_complete <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm, body_mass_g, flipper_length_mm)

penguins_scale <- penguins_complete %>% 
  select(ends_with('_mm'), body_mass_g) %>% 
  scale() # scale is creating a matrix that has a mean of zero and changing the sd = 1. 
```


### Estimate number of clusters
```{r}
number_estimate <- NbClust(penguins_scale, 
                           min.nc = 2, 
                           max.nc = 10, 
                           method = 'kmeans') #min and max c are cluster numbers. testing an algorithm. When the grpahs pop up look for the highest peaks. We see that 5 had tall peaks.. but the results in the consol tells us three? go with majority rule

fviz_nbclust(penguins_scale, FUNcluster = kmeans,
             method = 'wss',
             k.max = 10) # starts to level off around 3, especially after 5
```

### Run some k-means
```{r}
set.seed(123)
penguins_km <- kmeans(penguins_scale,
                      centers = 3,
                      iter.max = 10,
                      nstart = 25) # starting w 3 centers. throwing in the centroids then readjusiting where its going and readjusitng the centroids. its doing it 25 times and spitting out the lowest 

# penguins_km$size
# penguins_km$cluster
penguins_cl <- penguins_complete %>% 
  mutate(cluster_no = factor(penguins_km$cluster)) # have to change it to factors or else r will look at it as a continuous value
```

```{r}
ggplot(penguins_cl) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g,
                 color = cluster_no,
                 shape = species))

ggplot(penguins_cl) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm,
                 color = cluster_no,
                 shape = species)) +
  scale_colour_viridis_d()

penguins_cl %>% 
  select(species, cluster_no) %>% 
table() #putting these results in a table
```
### Hierarchicial Clustering 

#### Start with complete linkage
```{r}
#creating distance matrix
peng_dist <- dist(penguins_scale, method = 'euclidean') 

## Hierarchical clustering (complete linkage)
peng_hc_complete <- hclust(peng_dist, method = 'complete')

plot(peng_hc_complete, cex = .6, hang = -1)

### cut the tree  into three cluster
peng_cut_hc <- cutree(peng_hc_complete, 3)
table(peng_cut_hc, penguins_complete$species) #cluster 1 is mostly adelie, cluster 2 is mostly gentoo, cluster 3 is chinstrap
```
### World Bank data: read in and simplify
```{r}
wb_env <- read_csv(here::here('data/wb_env.csv'))

wb_ghg_20 <- wb_env %>% 
  slice_max(n = 20, ghg)

summary(wb_ghg_20)

wb_scaled <- wb_ghg_20 %>% 
  select(3:7) %>%  # grabbing column number 3,4,5,6,7
scale()
summary(wb_scaled)

rownames(wb_scaled) <- wb_ghg_20$name # for visualization bc we need numeric values. bc of those we lost the names of the countries so we just renamed the names of the rows with the name of the countries
```

### FInd the Euclidean distance
```{r}
euc_distance <- dist(wb_scaled, method = 'euclidean')
```


### Perform hierarchical clustering w/ complete linkage
```{r}
hc_complete <- hclust(euc_distance, method = 'complete')
plot(hc_complete, cex = .6, hang = -1)
```

### Perform heriarchical clustering by single linkage
```{r}
hc_single <- hclust(euc_distance, method = 'single')
plot(hc_single, cex = .6, hang = -1)
```

### Make a tanglegram
```{r}
dend_complete <- as.dendrogram(hc_complete)
dend_single <- as.dendrogram(hc_single) # making it a fancier dendrogram

tanglegram(dend_complete, dend_single) # combines the two dendrograms together. tells us which are grouped the same way

entanglement(dend_complete, dend_single) # gives us the amount of entanglement 

untangle(dend_complete,dend_single, method = 'step1side') %>% 
  entanglement() # takes a cluster and flips it to see if we could get a better line up of the clusters

untangle(dend_complete, dend_single, method = 'step1side') %>% 
  tanglegram(common_subtrees_color_branches = TRUE)

```


### Let's making a dendogram in ggplot
```{r}
ggdendrogram(hc_complete, rotate = TRUE ) +
  theme_minimal() +
  labs(x = 'Country')
```













